{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNK1mDHV7leCWbMPXUtfU/0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DIPANJAN001/Andrew-Ng-Machine-Learning-Notes/blob/master/SolarPowerManagementSystemFinal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "JZOF1U_PUrVh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from collections import deque\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "import random\n",
        "import numpy as np\n",
        "import gym\n",
        "from gym.utils import seeding\n",
        "from gym.spaces import Space, Discrete, MultiDiscrete,  Box\n",
        "from gym.spaces.space import Space\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import deque\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "from keras import metrics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MyEnv3(gym.Env):\n",
        "    \n",
        "      def __init__(self,seed=42):\n",
        "        np.random.seed(seed)\n",
        "        self.action_space = Discrete(4)\n",
        "        self.observation_space=MultiDiscrete([150,130,50])\n",
        "        self.battery_cap=30\n",
        "        self.max_battery_discharge=15\n",
        "        self.max_battery_charge=10\n",
        "        self.max_battery_capacity=50\n",
        "        self.state,self.reward, self.done, self.info= None, None, None,None\n",
        "        \n",
        "      def _seed(self, seed=None):\n",
        "        self.np_random, seed = seeding.np_random(seed)\n",
        "        return [seed]\n",
        "      def _render(self):\n",
        "        txt=\"state:\"+str(self.state)+\"reward\"+str(self.reward)+\"info:\"+str(self.info)\n",
        "        print(txt)\n",
        "\n",
        "      def _states(self):\n",
        "        observation_space=[]\n",
        "        return observation_space\n",
        "      def _reset(self):\n",
        "        self.battery_cap=30\n",
        "        self.state,self.reward,self.done,self.info=[30,0,0],0,False,{}\n",
        "        return [30,0,0]\n",
        "\n",
        "      def _step(self,action):\n",
        "        done=False\n",
        "        grid_export_=0\n",
        "        grid_import_=0\n",
        "        #load=np.random.randint(100,140)\n",
        "        #pv=np.random.randint(80,130)\n",
        "        battery_state=self.battery_cap\n",
        "        reward=0\n",
        "        data=[]\n",
        "        if action== 0:\n",
        "          grid_export_= max(pv-load-self.max_battery_charge,0) # grid export\n",
        "          self.battery_cap= max(min(self.battery_cap+min(self.max_battery_charge,pv-load),50),0)\n",
        "          grid_import_=0\n",
        "          info={\n",
        "              'grid_export':grid_export_,\n",
        "              'grid_import':grid_import_\n",
        "          }\n",
        "        if action== 1:\n",
        "          grid_export_= max(pv-load-self.max_battery_charge,0) # grid export\n",
        "          self.battery_cap= max(min(self.battery_cap+min(self.max_battery_charge,pv-load),50),0)\n",
        "          grid_import_=0\n",
        "          # only battery charge\n",
        "          info={\n",
        "              'grid_export':grid_export_,\n",
        "              'grid_import':grid_import_\n",
        "          }\n",
        "        if action== 2:\n",
        "            grid_export_=0\n",
        "            self.battery_cap= min(max(self.battery_cap-self.max_battery_discharge,0),50)# battery discharge\n",
        "            grid_import_=max(load-pv-self.max_battery_discharge,0)\n",
        "            info={\n",
        "              'grid_export':grid_export_,\n",
        "              'grid_import':grid_import_\n",
        "             }\n",
        "        if action== 3:\n",
        "          grid_export_=0\n",
        "          self.battery_cap= min(max(self.battery_cap-min(self.max_battery_discharge,load-pv),0),50)#change in battery\n",
        "          grid_import_=max(load-pv-self.max_battery_discharge,0)\n",
        "         #only battery discharge\n",
        "          info={\n",
        "              'grid_export':grid_export_,\n",
        "              'grid_import':grid_import_\n",
        "                    }     \n",
        "        \n",
        "        reward=grid_export_*8-grid_import_*10\n",
        "        \n",
        "        if(self.battery_cap<7.0):\n",
        "              done=True\n",
        "        #data.append([battery_state,load,pv,action,reward])\n",
        "        #state.append(load)\n",
        "        #state.append(pv)\n",
        "        return reward,self.battery_cap, done,info"
      ],
      "metadata": {
        "id": "mvhI4WLBVkBn"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''class Agent:\n",
        "    def __init__(self,state_size,action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=2000)\n",
        "        self.gamma = 0.95 #Discount Factor\n",
        "        self.epsilon = 1.0 # Exploration Rate: How much to act randomly, \n",
        "        self.epsilon_decay = 0.995\n",
        "        self.epsilon_min = 0.01\n",
        "        self.learning_rate = 0.001 \n",
        "        self.model = self._create_model()\n",
        "        \n",
        "    \n",
        "    def _create_model(self):\n",
        "        #Neural Network To Approximate Q-Value function\n",
        "        model = Sequential()\n",
        "        model.add(Dense(24,input_dim=self.state_size,activation='relu')) #1st Hidden Layer\n",
        "        model.add(Dense(24,activation='relu')) #2nd Hidden Layer\n",
        "        model.add(Dense(self.action_size,activation='linear'))\n",
        "        model.compile(loss='mse',optimizer=Adam(lr=self.learning_rate))\n",
        "        return model\n",
        "    \n",
        "    def remember(self,state,action,reward,done):\n",
        "        self.memory.append((state,action,reward,done)) #remembering previous experiences\n",
        "        \n",
        "    def act(self,state_):\n",
        "        # Exploration vs Exploitation\n",
        "        if np.random.rand()<=self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "        act_values = self.model.predict(state_) # predict reward value based upon current state\n",
        "        return np.argmax(act_values[0]) #Left or Right\n",
        "    \n",
        "    def train(self,batch_size=32): #method that trains NN with experiences sampled from memory\n",
        "        minibatch = random.sample(self.memory,batch_size)\n",
        "        for state,action,reward,next_state,done in minibatch:\n",
        "            \n",
        "            if not done: #boolean \n",
        "                target = reward + self.gamma*np.amax(self.model.predict(next_state)[0])\n",
        "            else:\n",
        "                target = reward\n",
        "            target_f = self.model.predict(state)\n",
        "            target_f[0][action] = target\n",
        "            self.model.fit(state,target_f,epochs=1,verbose=0) #single epoch, x =state, y = target_f, loss--> target_f - \n",
        "            \n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "    \n",
        "    def load(self,name):\n",
        "        self.model.load_weights(name)\n",
        "    def save(self,name):\n",
        "        self.model.save_weights(name)'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "rwoXuf_Wo4PZ",
        "outputId": "db020bba-43db-460e-a78b-4c4299deefd2"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"class Agent:\\n    def __init__(self,state_size,action_size):\\n        self.state_size = state_size\\n        self.action_size = action_size\\n        self.memory = deque(maxlen=2000)\\n        self.gamma = 0.95 #Discount Factor\\n        self.epsilon = 1.0 # Exploration Rate: How much to act randomly, \\n        self.epsilon_decay = 0.995\\n        self.epsilon_min = 0.01\\n        self.learning_rate = 0.001 \\n        self.model = self._create_model()\\n        \\n    \\n    def _create_model(self):\\n        #Neural Network To Approximate Q-Value function\\n        model = Sequential()\\n        model.add(Dense(24,input_dim=self.state_size,activation='relu')) #1st Hidden Layer\\n        model.add(Dense(24,activation='relu')) #2nd Hidden Layer\\n        model.add(Dense(self.action_size,activation='linear'))\\n        model.compile(loss='mse',optimizer=Adam(lr=self.learning_rate))\\n        return model\\n    \\n    def remember(self,state,action,reward,done):\\n        self.memory.append((state,action,reward,done)) #remembering previous experiences\\n        \\n    def act(self,state_):\\n        # Exploration vs Exploitation\\n        if np.random.rand()<=self.epsilon:\\n            return random.randrange(self.action_size)\\n        act_values = self.model.predict(state_) # predict reward value based upon current state\\n        return np.argmax(act_values[0]) #Left or Right\\n    \\n    def train(self,batch_size=32): #method that trains NN with experiences sampled from memory\\n        minibatch = random.sample(self.memory,batch_size)\\n        for state,action,reward,next_state,done in minibatch:\\n            \\n            if not done: #boolean \\n                target = reward + self.gamma*np.amax(self.model.predict(next_state)[0])\\n            else:\\n                target = reward\\n            target_f = self.model.predict(state)\\n            target_f[0][action] = target\\n            self.model.fit(state,target_f,epochs=1,verbose=0) #single epoch, x =state, y = target_f, loss--> target_f - \\n            \\n        if self.epsilon > self.epsilon_min:\\n            self.epsilon *= self.epsilon_decay\\n    \\n    def load(self,name):\\n        self.model.load_weights(name)\\n    def save(self,name):\\n        self.model.save_weights(name)\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env=MyEnv3()"
      ],
      "metadata": {
        "id": "ohwhbLD4ZYOe"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''agent = Agent(state_size=3,action_size=4)\n",
        "done = False\n",
        "state_size = 3\n",
        "action_size =4\n",
        "batch_size = 32'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wD6H9VEio6re",
        "outputId": "105e7bd7-3af8-4f0b-c7fb-ea315f4ee56f"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'agent = Agent(state_size=3,action_size=4)\\ndone = False\\nstate_size = 3\\naction_size =4\\nbatch_size = 32'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''agent = Agent(state_size, action_size) # initialise agent\n",
        "done = False\n",
        "for e in range(n_episodes):\n",
        "    state = env.reset()\n",
        "    state = np.reshape(state,[1,state_size])\n",
        "    \n",
        "    for time in range(5000):\n",
        "        env.render()\n",
        "        action = agent.act(state) #action is 0 or 1\n",
        "        next_state,reward,done,other_info = env.step(action) \n",
        "        reward = reward if not done else -10\n",
        "        next_state = np.reshape(next_state,[1,state_size])\n",
        "        agent.remember(state,action,reward,next_state,done)\n",
        "        state = next_state\n",
        "        \n",
        "        if done:\n",
        "            print(\"Game Episode :{}/{}, High Score:{},Exploration Rate:{:.2}\".format(e,n_episodes,time,agent.epsilon))\n",
        "            break\n",
        "            \n",
        "    if len(agent.memory)>batch_size:\n",
        "        agent.train(batch_size)\n",
        "    \n",
        "    if e%50==0:\n",
        "        agent.save(output_dir+\"weights_\"+'{:04d}'.format(e)+\".hdf5\")\n",
        "        \n",
        "env.close()'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "NO0V8k1PpIrp",
        "outputId": "e9000e54-6dd0-4625-b135-f794056da89c"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'agent = Agent(state_size, action_size) # initialise agent\\ndone = False\\nfor e in range(n_episodes):\\n    state = env.reset()\\n    state = np.reshape(state,[1,state_size])\\n    \\n    for time in range(5000):\\n        env.render()\\n        action = agent.act(state) #action is 0 or 1\\n        next_state,reward,done,other_info = env.step(action) \\n        reward = reward if not done else -10\\n        next_state = np.reshape(next_state,[1,state_size])\\n        agent.remember(state,action,reward,next_state,done)\\n        state = next_state\\n        \\n        if done:\\n            print(\"Game Episode :{}/{}, High Score:{},Exploration Rate:{:.2}\".format(e,n_episodes,time,agent.epsilon))\\n            break\\n            \\n    if len(agent.memory)>batch_size:\\n        agent.train(batch_size)\\n    \\n    if e%50==0:\\n        agent.save(output_dir+\"weights_\"+\\'{:04d}\\'.format(e)+\".hdf5\")\\n        \\nenv.close()'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#agent=Agent(3,4)"
      ],
      "metadata": {
        "id": "1bN1WuqHpgkZ"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#state=env._reset()"
      ],
      "metadata": {
        "id": "BC-7lBRnqDvI"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#state[0]"
      ],
      "metadata": {
        "id": "VN7TX6xTyiyH"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(state)"
      ],
      "metadata": {
        "id": "pd5sKnVIqGA2"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#state_=[]"
      ],
      "metadata": {
        "id": "-ojHzXXqzRUI"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''done=False\n",
        "for e in range(100):\n",
        "  score = 0\n",
        "  #load=np.random.randint(100,140)\n",
        "  #pv=np.random.randint(80,130)\n",
        "  state=env._reset()\n",
        "  done=False\n",
        "  state_=[]\n",
        "  state_.append(state[0])\n",
        "  while not done:\n",
        "    load=np.random.randint(100,140)\n",
        "    pv=np.random.randint(80,130)\n",
        "    state_.append(load)\n",
        "    state_.append(pv)\n",
        "    action = env.action_space.sample()\n",
        "    #action=agent.act(state_)\n",
        "    reward,observation2, done, info = env._step(action)\n",
        "    score+=round(reward)\n",
        "    #agent.remember(state_,action,reward,done)\n",
        "    state_[0]=[observation2]\n",
        "  print('episode:{} Profit:{}'.format(e,score))'''\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "dyCKXjS7plMG",
        "outputId": "3d7f0904-39e7-40f7-9ce4-e886dd649d3f"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"done=False\\nfor e in range(100):\\n  score = 0\\n  #load=np.random.randint(100,140)\\n  #pv=np.random.randint(80,130)\\n  state=env._reset()\\n  done=False\\n  state_=[]\\n  state_.append(state[0])\\n  while not done:\\n    load=np.random.randint(100,140)\\n    pv=np.random.randint(80,130)\\n    state_.append(load)\\n    state_.append(pv)\\n    action = env.action_space.sample()\\n    #action=agent.act(state_)\\n    reward,observation2, done, info = env._step(action)\\n    score+=round(reward)\\n    #agent.remember(state_,action,reward,done)\\n    state_[0]=[observation2]\\n  print('episode:{} Profit:{}'.format(e,score))\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''train_start = time.process_time()\n",
        "\n",
        "nb_episode = 15\n",
        "\n",
        "for idx, env in enumerate(building_environments):\n",
        "    print(f\"building {idx}\")\n",
        "    for i in range(nb_episode):\n",
        "        score = 0\n",
        "        done = False\n",
        "        observation = env.reset()\n",
        "        while not done:\n",
        "            action = agent.choose_action(observation)\n",
        "            observation_, reward, done, info = env.step(action)\n",
        "            score -= reward\n",
        "            agent.store_transition(observation, action, reward, observation_, done)\n",
        "            agent.train()\n",
        "            observation = observation_\n",
        "        print(f\"- episode : {i} | score : {score}\")\n",
        "train_end = time.process_time()\n",
        "train_frugality = train_end - train_start'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "IZg8eoIfnTb4",
        "outputId": "27904c11-455e-4855-9fdc-72cdbe5351ad"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'train_start = time.process_time()\\n\\nnb_episode = 15\\n\\nfor idx, env in enumerate(building_environments):\\n    print(f\"building {idx}\")\\n    for i in range(nb_episode):\\n        score = 0\\n        done = False\\n        observation = env.reset()\\n        while not done:\\n            action = agent.choose_action(observation)\\n            observation_, reward, done, info = env.step(action)\\n            score -= reward\\n            agent.store_transition(observation, action, reward, observation_, done)\\n            agent.train()\\n            observation = observation_\\n        print(f\"- episode : {i} | score : {score}\")\\ntrain_end = time.process_time()\\ntrain_frugality = train_end - train_start'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''for e in range(400): #Episode\n",
        "    #Play 20 episodes \n",
        "    score = 0\n",
        "    observation = env._reset()\n",
        "    #print(observation)\n",
        "    done=False\n",
        "    while not done:\n",
        "        #env.render()\n",
        "        action = agent.choose_action(observation)\n",
        "        #print(action)\n",
        "        observation1,reward,observation4,done,info = env._step(action)\n",
        "        print(observation1)\n",
        "        print(observation4)\n",
        "        score+=round(reward)\n",
        "        #print(info['load'],info['power'])\n",
        "      \n",
        "    print('episode:{} Profit:{}'.format(e,score))'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "M8WO3KblZydk",
        "outputId": "fa6e5926-0b03-4a44-c9ec-c456dc05d21a"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"for e in range(400): #Episode\\n    #Play 20 episodes \\n    score = 0\\n    observation = env._reset()\\n    #print(observation)\\n    done=False\\n    while not done:\\n        #env.render()\\n        action = agent.choose_action(observation)\\n        #print(action)\\n        observation1,reward,observation4,done,info = env._step(action)\\n        print(observation1)\\n        print(observation4)\\n        score+=round(reward)\\n        #print(info['load'],info['power'])\\n      \\n    print('episode:{} Profit:{}'.format(e,score))\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time # Necessary to evaluate frugality\n",
        "import numpy as np\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "cpOsfuM-3H5Z"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepQL(nn.Module):\n",
        "    def __init__(self, lr, input_dims, fc1_dims, fc2_dims, nb_actions):\n",
        "        super(DeepQL, self).__init__()\n",
        "        self.input_dims = 3\n",
        "        self.fc1_dims = fc1_dims\n",
        "        self.fc2_dims = fc2_dims\n",
        "        self.nb_actions = 4\n",
        "        self.fc1 = nn.Linear(self.input_dims, self.fc1_dims)\n",
        "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
        "        self.fc3 = nn.Linear(self.fc2_dims, self.nb_actions)\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
        "        self.loss = nn.MSELoss()\n",
        "        \n",
        "        if torch.cuda.is_available():       \n",
        "            self.device = torch.device(\"cuda\")\n",
        "        else:\n",
        "            self.device = torch.device(\"cpu\")\n",
        "        self.to(self.device)\n",
        "        \n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        actions = self.fc3(x)\n",
        "        \n",
        "        return actions"
      ],
      "metadata": {
        "id": "eSHbBp4R3OnH"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent():\n",
        "    def __init__(self, gamma, epsilon, lr, input_dims, batch_size, nb_actions,\n",
        "                max_mem_size = 100000, eps_end = 0.01, eps_dec = 5e-4):\n",
        "        \n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.batch_size = batch_size\n",
        "        self.eps_min = eps_end\n",
        "        self.eps_dec = eps_dec\n",
        "        self.lr = lr\n",
        "        self.action_space = [i for i in range(nb_actions)]\n",
        "        self.mem_size = max_mem_size\n",
        "        self.mem_counter = 0\n",
        "        \n",
        "        self.Q_eval = DeepQL(self.lr, nb_actions=nb_actions, input_dims=input_dims,\n",
        "                            fc1_dims=64, fc2_dims=64)\n",
        "        \n",
        "        self.state_memory = np.zeros((self.mem_size, input_dims), dtype=np.float32)\n",
        "        self.new_state_memory = np.zeros((self.mem_size, input_dims), dtype=np.float32)\n",
        "        \n",
        "        self.action_memory = np.zeros(self.mem_size, dtype=np.int32)\n",
        "        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
        "        self.terminal_memory = np.zeros(self.mem_size, dtype= np.bool)\n",
        "    \n",
        "    def store_transition(self, state, action ,reward, state_, done):\n",
        "        index = self.mem_counter % self.mem_size\n",
        "        self.state_memory[index] = state\n",
        "        self.action_memory[index] = action\n",
        "        self.reward_memory[index] = reward\n",
        "        self.new_state_memory[index] = state_\n",
        "        self.terminal_memory[index] = done\n",
        "        \n",
        "        self.mem_counter += 1\n",
        "    \n",
        "    def choose_action (self, observation):\n",
        "        if np.random.random() > self.epsilon:\n",
        "            state = torch.FloatTensor([observation]).to(self.Q_eval.device)\n",
        "            actions = self.Q_eval.forward(state)\n",
        "            action = torch.argmax(actions).item()\n",
        "        else:\n",
        "            action = np.random.choice(self.action_space)\n",
        "        \n",
        "        return action\n",
        "    def train(self):\n",
        "        if self.mem_counter < self.batch_size:\n",
        "            return\n",
        "        \n",
        "        self.Q_eval.optimizer.zero_grad()\n",
        "        \n",
        "        max_mem = min(self.mem_counter, self.mem_size)\n",
        "        batch = np.random.choice(max_mem,self.batch_size, replace=False)\n",
        "        \n",
        "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
        "        \n",
        "        state_batch = torch.tensor(self.state_memory[batch]).to(self.Q_eval.device)\n",
        "        new_state_batch = torch.tensor(self.new_state_memory[batch]).to(self.Q_eval.device)\n",
        "        reward_batch = torch.tensor(self.reward_memory[batch]).to(self.Q_eval.device)\n",
        "        terminal_batch = torch.tensor(self.terminal_memory[batch]).to(self.Q_eval.device)\n",
        "        \n",
        "        action_batch = self.action_memory[batch]\n",
        "        \n",
        "        q_eval = self.Q_eval.forward(state_batch)[batch_index, action_batch]\n",
        "        q_next = self.Q_eval.forward(new_state_batch)\n",
        "        q_next[terminal_batch] = 0.0\n",
        "        \n",
        "        q_target = reward_batch + self.gamma * torch.max(q_next,dim=1)[0]\n",
        "        \n",
        "        loss = self.Q_eval.loss(q_target, q_eval).to(self.Q_eval.device)\n",
        "        loss.backward()\n",
        "        self.Q_eval.optimizer.step()\n",
        "        \n",
        "        self.epsilon = self.epsilon - self.eps_dec if self.epsilon > self.eps_min else self.eps_min\n",
        "    "
      ],
      "metadata": {
        "id": "rljkaimf3stH"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Agent(gamma=0.99, epsilon=1.0, batch_size=128, nb_actions=4,\n",
        "             eps_end=0.01, input_dims=3, lr= 0.0001)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BjcVpU74ZlQ",
        "outputId": "10bc52b6-d8e0-4acf-a72e-08d05e28021a"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-102-7406a06ab070>:23: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  self.terminal_memory = np.zeros(self.mem_size, dtype= np.bool)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "done=False\n",
        "for e in range(100):\n",
        "  score = 0\n",
        "  #load=np.random.randint(100,140)\n",
        "  #pv=np.random.randint(80,130)\n",
        "  state=env._reset()\n",
        "  done=False\n",
        "  state_=[]\n",
        "  state_.append(state[0])\n",
        "  while not done:\n",
        "    load=np.random.randint(100,140)\n",
        "    pv=np.random.randint(80,130)\n",
        "    state_.append(load)\n",
        "    state_.append(pv)\n",
        "    print(state_)\n",
        "    #state1=state_\n",
        "    #action = env.action_space.sample()\n",
        "    action=agent.choose_action(state_)\n",
        "    print(action)\n",
        "    reward,observation2, done, info = env._step(action)\n",
        "    score+=round(reward)\n",
        "    print(observation2)\n",
        "    state_[0]=observation2\n",
        "    #agent.store_transition()\n",
        "  print('episode:{} Profit:{}'.format(e,score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7slcL3l5QqA",
        "outputId": "23b09791-0755-4dba-b3ea-2ef9a5ff98c0"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[30, 110, 128]\n",
            "3\n",
            "48\n",
            "[48, 110, 128, 104, 87]\n",
            "3\n",
            "33\n",
            "[33, 110, 128, 104, 87, 111, 117]\n",
            "3\n",
            "39\n",
            "[39, 110, 128, 104, 87, 111, 117, 104, 109]\n",
            "2\n",
            "24\n",
            "[24, 110, 128, 104, 87, 111, 117, 104, 109, 102, 90]\n",
            "1\n",
            "12\n",
            "[12, 110, 128, 104, 87, 111, 117, 104, 109, 102, 90, 126, 125]\n",
            "3\n",
            "11\n",
            "[11, 110, 128, 104, 87, 111, 117, 104, 109, 102, 90, 126, 125, 126, 121]\n",
            "1\n",
            "6\n",
            "episode:0 Profit:-20\n",
            "[30, 114, 109]\n",
            "1\n",
            "25\n",
            "[25, 114, 109, 136, 121]\n",
            "2\n",
            "10\n",
            "[10, 114, 109, 136, 121, 122, 85]\n",
            "3\n",
            "0\n",
            "episode:1 Profit:-220\n",
            "[30, 108, 128]\n",
            "2\n",
            "15\n",
            "[15, 108, 128, 116, 124]\n",
            "1\n",
            "23\n",
            "[23, 108, 128, 116, 124, 103, 84]\n",
            "0\n",
            "4\n",
            "episode:2 Profit:0\n",
            "[30, 112, 113]\n",
            "0\n",
            "31\n",
            "[31, 112, 113, 122, 82]\n",
            "3\n",
            "16\n",
            "[16, 112, 113, 122, 82, 120, 125]\n",
            "0\n",
            "21\n",
            "[21, 112, 113, 122, 82, 120, 125, 110, 118]\n",
            "0\n",
            "29\n",
            "[29, 112, 113, 122, 82, 120, 125, 110, 118, 127, 103]\n",
            "0\n",
            "5\n",
            "episode:3 Profit:-250\n",
            "[30, 133, 90]\n",
            "0\n",
            "0\n",
            "episode:4 Profit:0\n",
            "[30, 116, 114]\n",
            "3\n",
            "28\n",
            "[28, 116, 114, 121, 103]\n",
            "0\n",
            "10\n",
            "[10, 116, 114, 121, 103, 138, 100]\n",
            "0\n",
            "0\n",
            "episode:5 Profit:0\n",
            "[30, 116, 116]\n",
            "2\n",
            "15\n",
            "[15, 116, 116, 126, 113]\n",
            "0\n",
            "2\n",
            "episode:6 Profit:0\n",
            "[30, 111, 100]\n",
            "2\n",
            "15\n",
            "[15, 111, 100, 127, 84]\n",
            "3\n",
            "0\n",
            "episode:7 Profit:-280\n",
            "[30, 111, 85]\n",
            "2\n",
            "15\n",
            "[15, 111, 85, 125, 127]\n",
            "3\n",
            "17\n",
            "[17, 111, 85, 125, 127, 131, 85]\n",
            "1\n",
            "0\n",
            "episode:8 Profit:-110\n",
            "[30, 135, 117]\n",
            "1\n",
            "12\n",
            "[12, 135, 117, 123, 102]\n",
            "2\n",
            "0\n",
            "episode:9 Profit:-60\n",
            "[30, 116, 93]\n",
            "3\n",
            "15\n",
            "[15, 116, 93, 103, 86]\n",
            "3\n",
            "0\n",
            "episode:10 Profit:-100\n",
            "[30, 108, 87]\n",
            "0\n",
            "9\n",
            "[9, 108, 87, 116, 128]\n",
            "3\n",
            "21\n",
            "[21, 108, 87, 116, 128, 118, 117]\n",
            "0\n",
            "20\n",
            "[20, 108, 87, 116, 128, 118, 117, 112, 126]\n",
            "2\n",
            "5\n",
            "episode:11 Profit:0\n",
            "[30, 115, 129]\n",
            "0\n",
            "40\n",
            "[40, 115, 129, 121, 122]\n",
            "1\n",
            "41\n",
            "[41, 115, 129, 121, 122, 120, 95]\n",
            "0\n",
            "16\n",
            "[16, 115, 129, 121, 122, 120, 95, 123, 114]\n",
            "0\n",
            "7\n",
            "[7, 115, 129, 121, 122, 120, 95, 123, 114, 129, 126]\n",
            "1\n",
            "4\n",
            "episode:12 Profit:32\n",
            "[30, 107, 105]\n",
            "2\n",
            "15\n",
            "[15, 107, 105, 129, 86]\n",
            "3\n",
            "0\n",
            "episode:13 Profit:-280\n",
            "[30, 124, 116]\n",
            "0\n",
            "22\n",
            "[22, 124, 116, 112, 119]\n",
            "3\n",
            "29\n",
            "[29, 124, 116, 112, 119, 108, 80]\n",
            "1\n",
            "1\n",
            "episode:14 Profit:0\n",
            "[30, 139, 102]\n",
            "1\n",
            "0\n",
            "episode:15 Profit:0\n",
            "[30, 113, 83]\n",
            "1\n",
            "0\n",
            "episode:16 Profit:0\n",
            "[30, 110, 114]\n",
            "1\n",
            "34\n",
            "[34, 110, 114, 112, 124]\n",
            "0\n",
            "44\n",
            "[44, 110, 114, 112, 124, 118, 117]\n",
            "3\n",
            "43\n",
            "[43, 110, 114, 112, 124, 118, 117, 116, 113]\n",
            "3\n",
            "40\n",
            "[40, 110, 114, 112, 124, 118, 117, 116, 113, 137, 81]\n",
            "0\n",
            "0\n",
            "episode:17 Profit:16\n",
            "[30, 130, 113]\n",
            "3\n",
            "15\n",
            "[15, 130, 113, 122, 93]\n",
            "2\n",
            "0\n",
            "episode:18 Profit:-160\n",
            "[30, 126, 108]\n",
            "2\n",
            "15\n",
            "[15, 126, 108, 100, 96]\n",
            "0\n",
            "11\n",
            "[11, 126, 108, 100, 96, 112, 100]\n",
            "2\n",
            "0\n",
            "episode:19 Profit:-30\n",
            "[30, 124, 101]\n",
            "2\n",
            "15\n",
            "[15, 124, 101, 122, 111]\n",
            "1\n",
            "4\n",
            "episode:20 Profit:-80\n",
            "[30, 135, 87]\n",
            "3\n",
            "15\n",
            "[15, 135, 87, 131, 129]\n",
            "3\n",
            "13\n",
            "[13, 135, 87, 131, 129, 123, 94]\n",
            "2\n",
            "0\n",
            "episode:21 Profit:-470\n",
            "[30, 103, 92]\n",
            "3\n",
            "19\n",
            "[19, 103, 92, 117, 91]\n",
            "1\n",
            "0\n",
            "episode:22 Profit:0\n",
            "[30, 133, 97]\n",
            "2\n",
            "15\n",
            "[15, 133, 97, 110, 120]\n",
            "2\n",
            "0\n",
            "episode:23 Profit:-210\n",
            "[30, 130, 128]\n",
            "1\n",
            "28\n",
            "[28, 130, 128, 117, 111]\n",
            "0\n",
            "22\n",
            "[22, 130, 128, 117, 111, 122, 103]\n",
            "0\n",
            "3\n",
            "episode:24 Profit:0\n",
            "[30, 105, 84]\n",
            "3\n",
            "15\n",
            "[15, 105, 84, 128, 107]\n",
            "0\n",
            "0\n",
            "episode:25 Profit:-60\n",
            "[30, 118, 88]\n",
            "3\n",
            "15\n",
            "[15, 118, 88, 103, 115]\n",
            "2\n",
            "0\n",
            "episode:26 Profit:-150\n",
            "[30, 112, 98]\n",
            "3\n",
            "16\n",
            "[16, 112, 98, 111, 116]\n",
            "2\n",
            "1\n",
            "episode:27 Profit:0\n",
            "[30, 125, 108]\n",
            "2\n",
            "15\n",
            "[15, 125, 108, 124, 89]\n",
            "0\n",
            "0\n",
            "episode:28 Profit:-20\n",
            "[30, 120, 122]\n",
            "0\n",
            "32\n",
            "[32, 120, 122, 123, 123]\n",
            "3\n",
            "32\n",
            "[32, 120, 122, 123, 123, 124, 109]\n",
            "3\n",
            "17\n",
            "[17, 120, 122, 123, 123, 124, 109, 118, 98]\n",
            "1\n",
            "0\n",
            "episode:29 Profit:0\n",
            "[30, 116, 116]\n",
            "2\n",
            "15\n",
            "[15, 116, 116, 127, 120]\n",
            "3\n",
            "8\n",
            "[8, 116, 116, 127, 120, 117, 93]\n",
            "0\n",
            "0\n",
            "episode:30 Profit:0\n",
            "[30, 106, 120]\n",
            "0\n",
            "40\n",
            "[40, 106, 120, 112, 115]\n",
            "3\n",
            "43\n",
            "[43, 106, 120, 112, 115, 108, 88]\n",
            "1\n",
            "23\n",
            "[23, 106, 120, 112, 115, 108, 88, 121, 112]\n",
            "2\n",
            "8\n",
            "[8, 106, 120, 112, 115, 108, 88, 121, 112, 133, 88]\n",
            "2\n",
            "0\n",
            "episode:31 Profit:-268\n",
            "[30, 135, 97]\n",
            "1\n",
            "0\n",
            "episode:32 Profit:0\n",
            "[30, 120, 107]\n",
            "1\n",
            "17\n",
            "[17, 120, 107, 100, 115]\n",
            "2\n",
            "2\n",
            "episode:33 Profit:0\n",
            "[30, 134, 92]\n",
            "1\n",
            "0\n",
            "episode:34 Profit:0\n",
            "[30, 109, 123]\n",
            "1\n",
            "40\n",
            "[40, 109, 123, 112, 116]\n",
            "1\n",
            "44\n",
            "[44, 109, 123, 112, 116, 121, 96]\n",
            "1\n",
            "19\n",
            "[19, 109, 123, 112, 116, 121, 96, 134, 104]\n",
            "1\n",
            "0\n",
            "episode:35 Profit:32\n",
            "[30, 116, 91]\n",
            "3\n",
            "15\n",
            "[15, 116, 91, 128, 122]\n",
            "1\n",
            "9\n",
            "[9, 116, 91, 128, 122, 125, 117]\n",
            "2\n",
            "0\n",
            "episode:36 Profit:-100\n",
            "[30, 127, 95]\n",
            "3\n",
            "15\n",
            "[15, 127, 95, 127, 91]\n",
            "0\n",
            "0\n",
            "episode:37 Profit:-170\n",
            "[30, 129, 120]\n",
            "1\n",
            "21\n",
            "[21, 129, 120, 106, 111]\n",
            "0\n",
            "26\n",
            "[26, 129, 120, 106, 111, 132, 86]\n",
            "1\n",
            "0\n",
            "episode:38 Profit:0\n",
            "[30, 111, 95]\n",
            "0\n",
            "14\n",
            "[14, 111, 95, 135, 92]\n",
            "2\n",
            "0\n",
            "episode:39 Profit:-280\n",
            "[30, 108, 111]\n",
            "0\n",
            "33\n",
            "[33, 108, 111, 122, 104]\n",
            "0\n",
            "15\n",
            "[15, 108, 111, 122, 104, 121, 83]\n",
            "0\n",
            "0\n",
            "episode:40 Profit:0\n",
            "[30, 128, 113]\n",
            "0\n",
            "15\n",
            "[15, 128, 113, 114, 102]\n",
            "3\n",
            "3\n",
            "episode:41 Profit:0\n",
            "[30, 125, 119]\n",
            "0\n",
            "24\n",
            "[24, 125, 119, 131, 99]\n",
            "2\n",
            "9\n",
            "[9, 125, 119, 131, 99, 131, 107]\n",
            "0\n",
            "0\n",
            "episode:42 Profit:-170\n",
            "[30, 125, 123]\n",
            "0\n",
            "28\n",
            "[28, 125, 123, 125, 104]\n",
            "0\n",
            "7\n",
            "[7, 125, 123, 125, 104, 118, 90]\n",
            "3\n",
            "0\n",
            "episode:43 Profit:-130\n",
            "[30, 139, 83]\n",
            "3\n",
            "15\n",
            "[15, 139, 83, 110, 122]\n",
            "2\n",
            "0\n",
            "episode:44 Profit:-410\n",
            "[30, 137, 103]\n",
            "0\n",
            "0\n",
            "episode:45 Profit:0\n",
            "[30, 130, 120]\n",
            "1\n",
            "20\n",
            "[20, 130, 120, 104, 99]\n",
            "2\n",
            "5\n",
            "episode:46 Profit:0\n",
            "[30, 132, 129]\n",
            "1\n",
            "27\n",
            "[27, 132, 129, 114, 111]\n",
            "0\n",
            "24\n",
            "[24, 132, 129, 114, 111, 104, 85]\n",
            "1\n",
            "5\n",
            "episode:47 Profit:0\n",
            "[30, 117, 93]\n",
            "0\n",
            "6\n",
            "episode:48 Profit:0\n",
            "[30, 130, 109]\n",
            "1\n",
            "9\n",
            "[9, 130, 109, 128, 87]\n",
            "2\n",
            "0\n",
            "episode:49 Profit:-260\n",
            "[30, 132, 96]\n",
            "0\n",
            "0\n",
            "episode:50 Profit:0\n",
            "[30, 110, 110]\n",
            "1\n",
            "30\n",
            "[30, 110, 110, 115, 83]\n",
            "3\n",
            "15\n",
            "[15, 110, 110, 115, 83, 125, 91]\n",
            "0\n",
            "0\n",
            "episode:51 Profit:-170\n",
            "[30, 117, 98]\n",
            "1\n",
            "11\n",
            "[11, 117, 98, 117, 105]\n",
            "3\n",
            "0\n",
            "episode:52 Profit:0\n",
            "[30, 128, 113]\n",
            "2\n",
            "15\n",
            "[15, 128, 113, 115, 81]\n",
            "3\n",
            "0\n",
            "episode:53 Profit:-190\n",
            "[30, 112, 121]\n",
            "3\n",
            "39\n",
            "[39, 112, 121, 135, 126]\n",
            "2\n",
            "24\n",
            "[24, 112, 121, 135, 126, 138, 96]\n",
            "3\n",
            "9\n",
            "[9, 112, 121, 135, 126, 138, 96, 121, 108]\n",
            "0\n",
            "0\n",
            "episode:54 Profit:-270\n",
            "[30, 136, 89]\n",
            "3\n",
            "15\n",
            "[15, 136, 89, 127, 121]\n",
            "0\n",
            "9\n",
            "[9, 136, 89, 127, 121, 123, 121]\n",
            "1\n",
            "7\n",
            "[7, 136, 89, 127, 121, 123, 121, 107, 127]\n",
            "0\n",
            "17\n",
            "[17, 136, 89, 127, 121, 123, 121, 107, 127, 126, 108]\n",
            "0\n",
            "0\n",
            "episode:55 Profit:-240\n",
            "[30, 109, 115]\n",
            "3\n",
            "36\n",
            "[36, 109, 115, 126, 113]\n",
            "3\n",
            "23\n",
            "[23, 109, 115, 126, 113, 124, 108]\n",
            "2\n",
            "8\n",
            "[8, 109, 115, 126, 113, 124, 108, 119, 99]\n",
            "2\n",
            "0\n",
            "episode:56 Profit:-60\n",
            "[30, 134, 128]\n",
            "1\n",
            "24\n",
            "[24, 134, 128, 113, 96]\n",
            "3\n",
            "9\n",
            "[9, 134, 128, 113, 96, 128, 99]\n",
            "1\n",
            "0\n",
            "episode:57 Profit:-20\n",
            "[30, 103, 80]\n",
            "1\n",
            "7\n",
            "[7, 103, 80, 125, 124]\n",
            "2\n",
            "0\n",
            "episode:58 Profit:0\n",
            "[30, 109, 90]\n",
            "0\n",
            "11\n",
            "[11, 109, 90, 100, 96]\n",
            "3\n",
            "7\n",
            "[7, 109, 90, 100, 96, 128, 83]\n",
            "2\n",
            "0\n",
            "episode:59 Profit:-300\n",
            "[30, 120, 114]\n",
            "2\n",
            "15\n",
            "[15, 120, 114, 118, 124]\n",
            "2\n",
            "0\n",
            "episode:60 Profit:0\n",
            "[30, 106, 105]\n",
            "0\n",
            "29\n",
            "[29, 106, 105, 113, 83]\n",
            "3\n",
            "14\n",
            "[14, 106, 105, 113, 83, 114, 129]\n",
            "0\n",
            "24\n",
            "[24, 106, 105, 113, 83, 114, 129, 107, 120]\n",
            "3\n",
            "37\n",
            "[37, 106, 105, 113, 83, 114, 129, 107, 120, 138, 125]\n",
            "0\n",
            "24\n",
            "[24, 106, 105, 113, 83, 114, 129, 107, 120, 138, 125, 126, 83]\n",
            "2\n",
            "9\n",
            "[9, 106, 105, 113, 83, 114, 129, 107, 120, 138, 125, 126, 83, 136, 80]\n",
            "3\n",
            "0\n",
            "episode:61 Profit:-800\n",
            "[30, 126, 119]\n",
            "3\n",
            "23\n",
            "[23, 126, 119, 133, 108]\n",
            "1\n",
            "0\n",
            "episode:62 Profit:0\n",
            "[30, 103, 82]\n",
            "0\n",
            "9\n",
            "[9, 103, 82, 122, 102]\n",
            "2\n",
            "0\n",
            "episode:63 Profit:-50\n",
            "[30, 105, 116]\n",
            "3\n",
            "41\n",
            "[41, 105, 116, 106, 118]\n",
            "0\n",
            "50\n",
            "[50, 105, 116, 106, 118, 119, 82]\n",
            "1\n",
            "13\n",
            "[13, 105, 116, 106, 118, 119, 82, 103, 107]\n",
            "2\n",
            "0\n",
            "episode:64 Profit:16\n",
            "[30, 135, 126]\n",
            "0\n",
            "21\n",
            "[21, 135, 126, 122, 129]\n",
            "0\n",
            "28\n",
            "[28, 135, 126, 122, 129, 124, 127]\n",
            "1\n",
            "31\n",
            "[31, 135, 126, 122, 129, 124, 127, 135, 118]\n",
            "1\n",
            "14\n",
            "[14, 135, 126, 122, 129, 124, 127, 135, 118, 120, 100]\n",
            "3\n",
            "0\n",
            "episode:65 Profit:-50\n",
            "[30, 130, 121]\n",
            "2\n",
            "15\n",
            "[15, 130, 121, 133, 88]\n",
            "3\n",
            "0\n",
            "episode:66 Profit:-300\n",
            "[30, 109, 101]\n",
            "3\n",
            "22\n",
            "[22, 109, 101, 124, 100]\n",
            "0\n",
            "0\n",
            "episode:67 Profit:0\n",
            "[30, 100, 104]\n",
            "0\n",
            "34\n",
            "[34, 100, 104, 123, 81]\n",
            "3\n",
            "19\n",
            "[19, 100, 104, 123, 81, 138, 85]\n",
            "2\n",
            "4\n",
            "episode:68 Profit:-650\n",
            "[30, 138, 96]\n",
            "0\n",
            "0\n",
            "episode:69 Profit:0\n",
            "[30, 128, 88]\n",
            "1\n",
            "0\n",
            "episode:70 Profit:0\n",
            "[30, 119, 82]\n",
            "0\n",
            "0\n",
            "episode:71 Profit:0\n",
            "[30, 111, 127]\n",
            "2\n",
            "15\n",
            "[15, 111, 127, 100, 96]\n",
            "2\n",
            "0\n",
            "episode:72 Profit:0\n",
            "[30, 135, 111]\n",
            "3\n",
            "15\n",
            "[15, 135, 111, 117, 128]\n",
            "3\n",
            "26\n",
            "[26, 135, 111, 117, 128, 134, 119]\n",
            "0\n",
            "11\n",
            "[11, 135, 111, 117, 128, 134, 119, 113, 113]\n",
            "3\n",
            "11\n",
            "[11, 135, 111, 117, 128, 134, 119, 113, 113, 123, 108]\n",
            "0\n",
            "0\n",
            "episode:73 Profit:-90\n",
            "[30, 132, 128]\n",
            "0\n",
            "26\n",
            "[26, 132, 128, 100, 84]\n",
            "1\n",
            "10\n",
            "[10, 132, 128, 100, 84, 139, 92]\n",
            "1\n",
            "0\n",
            "episode:74 Profit:0\n",
            "[30, 134, 98]\n",
            "2\n",
            "15\n",
            "[15, 134, 98, 109, 104]\n",
            "3\n",
            "10\n",
            "[10, 134, 98, 109, 104, 122, 81]\n",
            "2\n",
            "0\n",
            "episode:75 Profit:-470\n",
            "[30, 136, 87]\n",
            "3\n",
            "15\n",
            "[15, 136, 87, 102, 129]\n",
            "2\n",
            "0\n",
            "episode:76 Profit:-340\n",
            "[30, 117, 95]\n",
            "0\n",
            "8\n",
            "[8, 117, 95, 111, 90]\n",
            "3\n",
            "0\n",
            "episode:77 Profit:-60\n",
            "[30, 116, 85]\n",
            "0\n",
            "0\n",
            "episode:78 Profit:0\n",
            "[30, 127, 93]\n",
            "3\n",
            "15\n",
            "[15, 127, 93, 132, 129]\n",
            "2\n",
            "0\n",
            "episode:79 Profit:-190\n",
            "[30, 121, 84]\n",
            "1\n",
            "0\n",
            "episode:80 Profit:0\n",
            "[30, 138, 83]\n",
            "3\n",
            "15\n",
            "[15, 138, 83, 106, 94]\n",
            "2\n",
            "0\n",
            "episode:81 Profit:-400\n",
            "[30, 118, 108]\n",
            "0\n",
            "20\n",
            "[20, 118, 108, 123, 97]\n",
            "0\n",
            "0\n",
            "episode:82 Profit:0\n",
            "[30, 107, 105]\n",
            "1\n",
            "28\n",
            "[28, 107, 105, 116, 81]\n",
            "0\n",
            "0\n",
            "episode:83 Profit:0\n",
            "[30, 127, 120]\n",
            "1\n",
            "23\n",
            "[23, 127, 120, 119, 120]\n",
            "0\n",
            "24\n",
            "[24, 127, 120, 119, 120, 111, 86]\n",
            "1\n",
            "0\n",
            "episode:84 Profit:0\n",
            "[30, 133, 104]\n",
            "1\n",
            "1\n",
            "episode:85 Profit:0\n",
            "[30, 123, 89]\n",
            "1\n",
            "0\n",
            "episode:86 Profit:0\n",
            "[30, 138, 106]\n",
            "1\n",
            "0\n",
            "episode:87 Profit:0\n",
            "[30, 119, 106]\n",
            "3\n",
            "17\n",
            "[17, 119, 106, 118, 99]\n",
            "1\n",
            "0\n",
            "episode:88 Profit:0\n",
            "[30, 129, 113]\n",
            "1\n",
            "14\n",
            "[14, 129, 113, 114, 106]\n",
            "0\n",
            "6\n",
            "episode:89 Profit:0\n",
            "[30, 131, 127]\n",
            "2\n",
            "15\n",
            "[15, 131, 127, 135, 81]\n",
            "0\n",
            "0\n",
            "episode:90 Profit:0\n",
            "[30, 118, 105]\n",
            "0\n",
            "17\n",
            "[17, 118, 105, 114, 101]\n",
            "0\n",
            "4\n",
            "episode:91 Profit:0\n",
            "[30, 121, 115]\n",
            "0\n",
            "24\n",
            "[24, 121, 115, 127, 80]\n",
            "3\n",
            "9\n",
            "[9, 121, 115, 127, 80, 106, 128]\n",
            "2\n",
            "0\n",
            "episode:92 Profit:-320\n",
            "[30, 100, 89]\n",
            "0\n",
            "19\n",
            "[19, 100, 89, 131, 111]\n",
            "1\n",
            "0\n",
            "episode:93 Profit:0\n",
            "[30, 116, 89]\n",
            "1\n",
            "3\n",
            "episode:94 Profit:0\n",
            "[30, 134, 100]\n",
            "3\n",
            "15\n",
            "[15, 134, 100, 114, 83]\n",
            "0\n",
            "0\n",
            "episode:95 Profit:-190\n",
            "[30, 126, 105]\n",
            "3\n",
            "15\n",
            "[15, 126, 105, 116, 122]\n",
            "1\n",
            "21\n",
            "[21, 126, 105, 116, 122, 119, 124]\n",
            "0\n",
            "26\n",
            "[26, 126, 105, 116, 122, 119, 124, 122, 126]\n",
            "2\n",
            "11\n",
            "[11, 126, 105, 116, 122, 119, 124, 122, 126, 132, 125]\n",
            "3\n",
            "4\n",
            "episode:96 Profit:-60\n",
            "[30, 133, 119]\n",
            "1\n",
            "16\n",
            "[16, 133, 119, 102, 112]\n",
            "1\n",
            "26\n",
            "[26, 133, 119, 102, 112, 129, 127]\n",
            "1\n",
            "24\n",
            "[24, 133, 119, 102, 112, 129, 127, 137, 98]\n",
            "2\n",
            "9\n",
            "[9, 133, 119, 102, 112, 129, 127, 137, 98, 112, 125]\n",
            "1\n",
            "19\n",
            "[19, 133, 119, 102, 112, 129, 127, 137, 98, 112, 125, 100, 106]\n",
            "2\n",
            "4\n",
            "episode:97 Profit:-216\n",
            "[30, 116, 88]\n",
            "2\n",
            "15\n",
            "[15, 116, 88, 124, 118]\n",
            "3\n",
            "9\n",
            "[9, 116, 88, 124, 118, 132, 116]\n",
            "3\n",
            "0\n",
            "episode:98 Profit:-140\n",
            "[30, 120, 81]\n",
            "2\n",
            "15\n",
            "[15, 120, 81, 137, 119]\n",
            "3\n",
            "0\n",
            "episode:99 Profit:-270\n"
          ]
        }
      ]
    }
  ]
}