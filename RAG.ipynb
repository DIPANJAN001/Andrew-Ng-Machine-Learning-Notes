{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMKLIRZ2ZfSpnPVXwUfGy0q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DIPANJAN001/Andrew-Ng-Machine-Learning-Notes/blob/master/RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6uTxN9xuuCro",
        "outputId": "befedc1c-d8e9-4a59-8d6e-6081d67f59c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Text1', 'Text1', 'Text2'), ('Text1', 'Text1', 'Text4'), ('Text1', 'Text1', 'Text5'), ('Text1', 'Text3', 'Text2'), ('Text1', 'Text3', 'Text4')]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Sample data\n",
        "data = {\n",
        "    \"claim_type\": [\"Auto\", \"Health\", \"Auto\", \"Health\", \"Home\"],\n",
        "    \"document\": [\"Text1\", \"Text2\", \"Text3\", \"Text4\", \"Text5\"]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Create triplets\n",
        "triplets = []\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    anchor_claim_type = row[\"claim_type\"]\n",
        "    anchor_document = row[\"document\"]\n",
        "\n",
        "    # Positive examples: Documents with the same claim type\n",
        "    positive_examples = df[df[\"claim_type\"] == anchor_claim_type]\n",
        "\n",
        "    # Negative examples: Documents with different claim types\n",
        "    negative_examples = df[df[\"claim_type\"] != anchor_claim_type]\n",
        "\n",
        "    for _, pos_row in positive_examples.iterrows():\n",
        "        for _, neg_row in negative_examples.iterrows():\n",
        "            triplets.append((anchor_document, pos_row[\"document\"], neg_row[\"document\"]))\n",
        "\n",
        "# Display a few triplets\n",
        "print(triplets[:5])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Load a pre-trained SBERT model (e.g., 'paraphrase-MiniLM-L6-v2')\n",
        "sbert_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "\n",
        "# Define the Siamese network architecture\n",
        "class SiameseNetwork(nn.Module):\n",
        "    def __init__(self, sbert_model):\n",
        "        super(SiameseNetwork, self).__init__()\n",
        "        self.sbert_model = sbert_model\n",
        "\n",
        "    def forward_one(self, x):\n",
        "        # Forward pass through SBERT model\n",
        "        return self.sbert_model.encode(x, convert_to_tensor=True)\n",
        "\n",
        "    def forward(self, input1, input2):\n",
        "        # Forward pass for two input samples\n",
        "        output1 = self.forward_one(input1)\n",
        "        output2 = self.forward_one(input2)\n",
        "        return output1, output2\n",
        "\n",
        "# Define the triplet loss function\n",
        "class TripletLoss(nn.Module):\n",
        "    def __init__(self, margin=1.0):\n",
        "        super(TripletLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, anchor, positive, negative):\n",
        "        distance_positive = 1.0 - util.pytorch_cos_sim(anchor, positive)\n",
        "        distance_negative = 1.0 - util.pytorch_cos_sim(anchor, negative)\n",
        "        loss = torch.relu(distance_positive - distance_negative + self.margin)\n",
        "        return loss.mean()\n",
        "\n",
        "# Create the Siamese network\n",
        "siamese_net = SiameseNetwork(sbert_model)\n",
        "triplet_loss = TripletLoss()\n",
        "\n",
        "# Define optimizer and learning rate\n",
        "optimizer = optim.Adam(siamese_net.parameters(), lr=2e-5)\n",
        "\n",
        "# Dummy data (replace with your data loading logic)\n",
        "anchor_input = [\"Query 1\", \"Query 2\", \"Query 3\"]\n",
        "positive_input = [\"Positive 1\", \"Positive 2\", \"Positive 3\"]\n",
        "negative_input = [\"Negative 1\", \"Negative 2\", \"Negative 3\"]\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    anchor_output, positive_output = siamese_net(anchor_input, positive_input)\n",
        "    anchor_output, negative_output = siamese_net(anchor_input, negative_input)\n",
        "\n",
        "    loss = triplet_loss(anchor_output, positive_output, negative_output)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}\")\n",
        "\n",
        "# After training, you can use the siamese_net to obtain SBERT embeddings for queries and documents.\n"
      ],
      "metadata": {
        "id": "2iRYHc5Rv_wq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for anchor_doc, positive_doc, negative_doc in triplets:\n",
        "        # Encode the anchor, positive, and negative documents using your model\n",
        "        anchor_embedding = model.encode(anchor_doc)\n",
        "        positive_embedding = model.encode(positive_doc)\n",
        "        negative_embedding = model.encode(negative_doc)\n",
        "\n",
        "        # Compute the triplet loss\n",
        "        loss = triplet_loss(anchor_embedding, positive_embedding, negative_embedding)\n",
        "\n",
        "        # Backpropagate and update model parameters\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Print the loss for monitoring\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}\")"
      ],
      "metadata": {
        "id": "626ca52Zux3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = {\n",
        "    \"query\": [\"Fire damage claim\", \"Injury compensation claim\", \"Healthcare reimbursement claim\"],\n",
        "    \"document\": [\"Text1\", \"Text2\", \"Text3\"]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Create triplets\n",
        "triplets = []\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    anchor_query = row[\"query\"]\n",
        "    anchor_document = row[\"document\"]\n",
        "\n",
        "    # Positive examples: Documents with the same category/query\n",
        "    positive_examples = df[df[\"query\"] == anchor_query]\n",
        "\n",
        "    # Negative examples: Documents with different categories/queries\n",
        "    negative_examples = df[df[\"query\"] != anchor_query]\n",
        "\n",
        "    for _, pos_row in positive_examples.iterrows():\n",
        "        for _, neg_row in negative_examples.iterrows():\n",
        "            triplets.append((anchor_query, pos_row[\"document\"], neg_row[\"document\"]))\n",
        "\n",
        "# Display a few triplets\n",
        "print(triplets[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-1JBca2u395",
        "outputId": "8c57f820-6fce-4ddd-dbda-f44c6ab7cb52"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Fire damage claim', 'Text1', 'Text2'), ('Fire damage claim', 'Text1', 'Text3'), ('Injury compensation claim', 'Text2', 'Text1'), ('Injury compensation claim', 'Text2', 'Text3'), ('Healthcare reimbursement claim', 'Text3', 'Text1')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# Sample data with claim types replaced by random queries\n",
        "data = {\n",
        "    \"claim_type\": [\"Random Query\", \"Random Query\", \"Random Query\", \"Random Query\", \"Random Query\"],\n",
        "    \"document\": [\"Text1\", \"Text2\", \"Text3\", \"Text4\", \"Text5\"]\n",
        "}\n",
        "\n",
        "# Example queries for each category\n",
        "fire_queries = [\"Fire damage claim\", \"Filing a fire insurance claim\", \"Fire accident compensation\", ...]\n",
        "casualty_queries = [\"Personal injury claim procedure\", \"Car accident compensation process\", ...]\n",
        "ah_queries = [\"Health insurance claim submission\", \"Hospitalization expenses reimbursement claim\", ...]\n",
        "\n",
        "# Create lists to store pairs of queries and documents\n",
        "query_document_pairs = []\n",
        "\n",
        "# Iterate through each category and pair queries with documents\n",
        "for category in [\"Fire\", \"Casualty\", \"Accident & Health\"]:\n",
        "    if category == \"Fire\":\n",
        "        queries = fire_queries\n",
        "    elif category == \"Casualty\":\n",
        "        queries = casualty_queries\n",
        "    else:\n",
        "        queries = ah_queries\n",
        "\n",
        "    for query in queries:\n",
        "        for document in data[\"document\"]:\n",
        "            query_document_pairs.append((query, document))\n",
        "\n",
        "# Create a DataFrame with query-document pairs\n",
        "df = pd.DataFrame(query_document_pairs, columns=[\"query\", \"document\"])\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVzWhLyVu415",
        "outputId": "1754c864-cd9c-4af8-888f-84a8541bea91"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                           query document\n",
            "0                              Fire damage claim    Text1\n",
            "1                              Fire damage claim    Text2\n",
            "2                              Fire damage claim    Text3\n",
            "3                              Fire damage claim    Text4\n",
            "4                              Fire damage claim    Text5\n",
            "5                  Filing a fire insurance claim    Text1\n",
            "6                  Filing a fire insurance claim    Text2\n",
            "7                  Filing a fire insurance claim    Text3\n",
            "8                  Filing a fire insurance claim    Text4\n",
            "9                  Filing a fire insurance claim    Text5\n",
            "10                    Fire accident compensation    Text1\n",
            "11                    Fire accident compensation    Text2\n",
            "12                    Fire accident compensation    Text3\n",
            "13                    Fire accident compensation    Text4\n",
            "14                    Fire accident compensation    Text5\n",
            "15                                      Ellipsis    Text1\n",
            "16                                      Ellipsis    Text2\n",
            "17                                      Ellipsis    Text3\n",
            "18                                      Ellipsis    Text4\n",
            "19                                      Ellipsis    Text5\n",
            "20               Personal injury claim procedure    Text1\n",
            "21               Personal injury claim procedure    Text2\n",
            "22               Personal injury claim procedure    Text3\n",
            "23               Personal injury claim procedure    Text4\n",
            "24               Personal injury claim procedure    Text5\n",
            "25             Car accident compensation process    Text1\n",
            "26             Car accident compensation process    Text2\n",
            "27             Car accident compensation process    Text3\n",
            "28             Car accident compensation process    Text4\n",
            "29             Car accident compensation process    Text5\n",
            "30                                      Ellipsis    Text1\n",
            "31                                      Ellipsis    Text2\n",
            "32                                      Ellipsis    Text3\n",
            "33                                      Ellipsis    Text4\n",
            "34                                      Ellipsis    Text5\n",
            "35             Health insurance claim submission    Text1\n",
            "36             Health insurance claim submission    Text2\n",
            "37             Health insurance claim submission    Text3\n",
            "38             Health insurance claim submission    Text4\n",
            "39             Health insurance claim submission    Text5\n",
            "40  Hospitalization expenses reimbursement claim    Text1\n",
            "41  Hospitalization expenses reimbursement claim    Text2\n",
            "42  Hospitalization expenses reimbursement claim    Text3\n",
            "43  Hospitalization expenses reimbursement claim    Text4\n",
            "44  Hospitalization expenses reimbursement claim    Text5\n",
            "45                                      Ellipsis    Text1\n",
            "46                                      Ellipsis    Text2\n",
            "47                                      Ellipsis    Text3\n",
            "48                                      Ellipsis    Text4\n",
            "49                                      Ellipsis    Text5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import random\n",
        "\n",
        "# Function to generate information retrieval queries\n",
        "\n",
        "# Function to generate queries for each claim type\n",
        "def generate_claim_type_queries(claim_type):\n",
        "    if claim_type == \"Fire\":\n",
        "        queries = [\n",
        "            \"Search for relevant details in fire insurance claims.\",\n",
        "            \"Retrieve key information from documents related to fire incidents.\",\n",
        "            \"Locate specific data within fire damage claim records.\",\n",
        "            \"Find critical facts in fire insurance documentation.\",\n",
        "            \"Access important insights from fire claim reports.\",\n",
        "            \"Search for relevant content in documents related to fire claims.\",\n",
        "            \"Retrieve actionable intelligence from fire insurance records.\",\n",
        "            \"Locate relevant data points within fire damage claim documents.\",\n",
        "            \"Access pertinent information within fire insurance policies.\",\n",
        "            \"Seek out valuable data in the context of fire claims.\"\n",
        "        ]\n",
        "    elif claim_type == \"Casualty\":\n",
        "        queries = [\n",
        "            \"Search for relevant details in casualty insurance claims.\",\n",
        "            \"Retrieve key information from documents related to personal injuries.\",\n",
        "            \"Locate specific data within injury compensation claim records.\",\n",
        "            \"Find critical facts in casualty insurance documentation.\",\n",
        "            \"Access important insights from personal injury claim reports.\",\n",
        "            \"Search for relevant content in documents related to casualty claims.\",\n",
        "            \"Retrieve actionable intelligence from injury compensation records.\",\n",
        "            \"Locate relevant data points within casualty insurance documents.\",\n",
        "            \"Access pertinent information within liability claim records.\",\n",
        "            \"Seek out valuable data in the context of casualty claims.\"\n",
        "        ]\n",
        "    elif claim_type == \"Accident & Health\":\n",
        "        queries = [\n",
        "            \"Search for relevant details in accident & health insurance claims.\",\n",
        "            \"Retrieve key information from documents related to healthcare claims.\",\n",
        "            \"Locate specific data within health insurance reimbursement records.\",\n",
        "            \"Find critical facts in accident & health insurance documentation.\",\n",
        "            \"Access important insights from healthcare claim reports.\",\n",
        "            \"Search for relevant content in documents related to A&H claims.\",\n",
        "            \"Retrieve actionable intelligence from accident & health insurance records.\",\n",
        "            \"Locate relevant data points within healthcare reimbursement documents.\",\n",
        "            \"Access pertinent information within accident & health insurance policies.\",\n",
        "            \"Seek out valuable data in the context of A&H claims.\"\n",
        "        ]\n",
        "    else:\n",
        "        queries = []\n",
        "    return queries\n",
        "\n",
        "# Example: Generate information retrieval queries\n",
        "\n",
        "\n",
        "# Example: Generate queries for the \"Fire\" claim type\n",
        "fire_queries = generate_claim_type_queries(\"Fire\")\n",
        "\n",
        "# Example: Generate queries for the \"Casualty\" claim type\n",
        "casualty_queries = generate_claim_type_queries(\"Casualty\")\n",
        "\n",
        "# Example: Generate queries for the \"Accident & Health\" claim type\n",
        "ah_queries = generate_claim_type_queries(\"Accident & Health\")\n",
        "\n",
        "# Print and use the generated queries as needed\n",
        "print(\"Information Retrieval Queries:\")\n",
        "for i, query in enumerate(info_retrieval_queries, start=1):\n",
        "    print(f\"{i}. {query}\")\n",
        "print(\"\\nFire Claim Queries:\")\n",
        "for i, query in enumerate(fire_queries, start=1):\n",
        "    print(f\"{i}. {query}\")\n",
        "print(\"\\nCasualty Claim Queries:\")\n",
        "for i, query in enumerate(casualty_queries, start=1):\n",
        "    print(f\"{i}. {query}\")\n",
        "print(\"\\nAccident & Health Claim Queries:\")\n",
        "for i, query in enumerate(ah_queries, start=1):\n",
        "    print(f\"{i}. {query}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "fMl51bA4vAgH",
        "outputId": "ee28192f-22b8-4686-a128-df8d4c5181ca"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Information Retrieval Queries:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-7434d9ae8870>\u001b[0m in \u001b[0;36m<cell line: 65>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;31m# Print and use the generated queries as needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Information Retrieval Queries:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo_retrieval_queries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{i}. {query}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nFire Claim Queries:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'info_retrieval_queries' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Assuming chunk_embeddings is a list of embeddings for each chunk\n",
        "chunk_embeddings = [chunk1_embedding, chunk2_embedding, ...]\n",
        "\n",
        "# Calculate the mean of embeddings\n",
        "document_embedding = np.mean(chunk_embeddings, axis=0)"
      ],
      "metadata": {
        "id": "Xa6x8RayvY2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SiameseNetwork(nn.Module):\n",
        "    def __init__(self, sbert_model):\n",
        "        super(SiameseNetwork, self).__init__()\n",
        "        self.sbert_model = sbert_model\n",
        "\n",
        "    def forward_one(self, x):\n",
        "        # Forward pass through SBERT model\n",
        "        return self.sbert_model.encode(x, convert_to_tensor=True)\n",
        "\n",
        "    def forward(self, input1, input2):\n",
        "        # Forward pass for two input samples\n",
        "        output1 = self.forward_one(input1)\n",
        "        output2 = self.forward_one(input2)\n",
        "        return output1, output2"
      ],
      "metadata": {
        "id": "22KRzyZvwTfS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}